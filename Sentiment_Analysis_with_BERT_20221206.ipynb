{"cells":[{"cell_type":"markdown","id":"infectious-buddy","metadata":{"id":"infectious-buddy"},"source":["# Sentiment Analysis with BERT\n","\n","The objective is to judge the sentiment label of a sentence.\n","It consists of 3 labels, `positive`, `negative`, and `neutral`.\n","A dataset that contains sentences with the corresponding sentiment label is provided, and you have to use BERT and train a sentence classifier with this dataset.\n","\n","As to the implementatin, we will introduce you the [ðŸ¤— transformers](https://huggingface.co/) library, which is mantained by huggingface company, as the training framework this week. [Pytorch](https://pytorch.org/) is used as the deep learning backend in this tutorial."]},{"cell_type":"markdown","id":"given-speaker","metadata":{"id":"given-speaker"},"source":["## Step 1: Prepare your environment\n","\n","### 1.1 Create a new environment with conda\n","\n","Again, we highly recommend you to install all packages with a virtual environment manager, like [conda](https://docs.conda.io/projects/conda/en/latest/user-guide/getting-started.html), to prevent version conflicts of different packages.\n","\n","```\n","# Create a new VM\n","conda create -y -n sentiment-analysis python=3.9\n","\n","# Activate the VM\n","conda activate sentiment-analysis\n","```"]},{"cell_type":"markdown","id":"f08c0036","metadata":{"id":"f08c0036"},"source":["### 1.2 Install python packages\n","\n","Command:\n","```\n","pip install -r requirements.txt\n","```\n","\n","Dependencies:\n","\n","1. `numpy`: for matrix operation\n","2. `scikit-learn`: for label encoding\n","3. `datasets`: for data preparation\n","4. `transformers`: for model loading and finetuing\n","5. `pytorch`: the backend DL framework"]},{"cell_type":"markdown","id":"8e18cc24","metadata":{"id":"8e18cc24"},"source":["### 1.3 Select GPU(s) for your backend\n","\n","Select your GPU.\n","Note that this should be set before you load tensorflow or pytorch."]},{"cell_type":"code","execution_count":1,"id":"a2c2f1f2","metadata":{"id":"a2c2f1f2"},"outputs":[],"source":["import os\n","\n","os.environ['CUDA_VISIBLE_DEVICES'] = '0'"]},{"cell_type":"markdown","id":"19aa1f9c","metadata":{"id":"19aa1f9c"},"source":["#### 1.4 Check Pytorch"]},{"cell_type":"code","execution_count":2,"id":"254c8721","metadata":{"id":"254c8721","outputId":"9e140859-65d8-4b10-9eeb-2507fae85e4c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/nlplab/kedy/miniconda3/envs/sentiment-analysis/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]},{"data":{"text/plain":["True"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","torch.cuda.is_available()"]},{"cell_type":"markdown","id":"small-radio","metadata":{"id":"small-radio"},"source":["## Step 2: Prepare the dataset\n","\n","Before starting the training, we need to load and process our dataset."]},{"cell_type":"markdown","id":"prerequisite-woman","metadata":{"id":"prerequisite-woman"},"source":["### 2.1 Load data\n","\n","Library, `datasets` is a package provided by huggingface.\n","\n","It contains many public datasets online and can help us with the data processing.\n","\n","We can use `load_dataset` function to read the input `.csv` file.\n","\n","Reference:\n"," - [Official datasets document](https://huggingface.co/docs/datasets)\n"," - [datasets.load_dataset](https://huggingface.co/docs/datasets/loading.html)"]},{"cell_type":"code","execution_count":3,"id":"personalized-tourist","metadata":{"id":"personalized-tourist"},"outputs":[],"source":["import os\n","from datasets import load_dataset"]},{"cell_type":"code","execution_count":4,"id":"minute-tension","metadata":{"colab":{"referenced_widgets":["cae0c659f3874b81999a4933da4f8d9b"]},"id":"minute-tension","outputId":"6b96ccb3-8993-43e8-d9a2-21afccb551d3"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using custom data configuration default-b6e650acb670b9ab\n","Found cached dataset csv (/home/nlplab/kedy/.cache/huggingface/datasets/csv/default-b6e650acb670b9ab/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 419.10it/s]\n"]}],"source":["# TODO: Setup path and filename\n","path_to_folder = ...\n","filename = ...\n","dataset = load_dataset('csv', data_files = os.path.join(..., ...))"]},{"cell_type":"markdown","id":"91cfe36a","metadata":{},"source":["### 2.2 Check loaded data structure"]},{"cell_type":"code","execution_count":5,"id":"3ccd2ace","metadata":{"id":"3ccd2ace","outputId":"0ea17463-2d85-4eb1-9d15-bd9d6ec85e39"},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['label', 'text'],\n","    num_rows: 20632\n","})\n","{'label': 'neutral', 'text': 'Order Go Set a Watchman in store or through our website before Tuesday and get it half price! #GSAW @GSAWatchmanBook https://t.co/KET6EGD1an'}\n","[\"Picturehouse's, Pink Floyd's, 'Roger Waters: The Walll - opening 29 Sept is now making waves. Watch the trailer on Rolling Stone - look...\", 'Order Go Set a Watchman in store or through our website before Tuesday and get it half price! #GSAW @GSAWatchmanBook https://t.co/KET6EGD1an', 'If these runway renovations at the airport prevent me from seeing Taylor Swift on Monday, Bad Blood will have a new meaning.', 'If you could ask an onstage interview question at Miss USA tomorrow, what would it be?', 'A portion of book sales from our Harper Lee/Go Set a Watchman release party on Mon. 7/13 will support @CAP_Tulsa and the great work they do.']\n"]}],"source":["print(dataset['train'])"]},{"cell_type":"code","execution_count":null,"id":"d3d38b23","metadata":{},"outputs":[],"source":["# [ Practice ] Check first data\n","print(dataset['train'][...])"]},{"cell_type":"code","execution_count":null,"id":"094f120c","metadata":{},"outputs":[],"source":["# [ Practice ] Check first 5 texts\n","print(dataset['train'][...][:5])"]},{"cell_type":"markdown","id":"dabc16be","metadata":{"id":"dabc16be"},"source":["## Step 3: Preprocessing\n","\n","Before put into the model, texts should be tokenized, embedded, and padded.\n","\n","Here we use [BERT](https://arxiv.org/abs/1810.04805) (**B**idirectional **E**ncoder **R**epresentations from **T**ransformers), a language model proposed by Google AI in 2018.\n","\n","It's one of the most popular models used in NLP area.\n","\n","However, we will not directly use BERT in this tutorial, because it's large and needs plenty of time to train.\n","\n","Instead, we are using [DistilBert](https://medium.com/huggingface/distilbert-8cf3380435b5). \n","\n","DistilBERT is a distilled (è’¸é¤¾) version of BERT that is much more light-weighted than original model while reserving 95% of its original accuracy, which makes it perfect for our task today."]},{"cell_type":"markdown","id":"2b021563","metadata":{},"source":["### 3.1 Specify the language model"]},{"cell_type":"code","execution_count":6,"id":"4074c389","metadata":{},"outputs":[],"source":["# Available models can be found here: https://huggingface.co/models\n","# DistilBERT base model (uncased): https://huggingface.co/distilbert-base-uncased\n","# TODO: Setup the model name\n","MODEL_NAME = ..."]},{"cell_type":"markdown","id":"1302e662","metadata":{"id":"1302e662"},"source":["### 3.2 Sentence processing with tokenizer\n","\n","Different pre-trained language models may have their own preprocessing models, and that's why we should use the tokenizers trained along with that model.\n","\n","In our case, we are using distilBERT, so we should use the distilBERT tokenizer.  \n","\n","With huggingface, loading different tokenizer is extremely easy: just import the `AutoTokenizer` from `transformers` and tell it what model you plan to use, and it will handle everything for you."]},{"cell_type":"code","execution_count":7,"id":"831e6af0","metadata":{"id":"831e6af0"},"outputs":[],"source":["from transformers import AutoTokenizer # For tokenization\n","\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"]},{"cell_type":"markdown","id":"6323c2f3","metadata":{"id":"6323c2f3"},"source":["### 3.3 Play with BERTTokenizer"]},{"cell_type":"code","execution_count":8,"id":"3f39baab","metadata":{"id":"3f39baab","outputId":"9eef456f-231a-4567-a0ba-d3f4703fe488"},"outputs":[{"data":{"text/plain":["{'input_ids': [101, 2023, 2061, 1011, 2170, 1000, 3819, 3944, 1000, 2001, 2061, 15640, 1010, 2004, 2092, 2004, 12532, 4648, 4726, 2149, 2013, 2746, 2000, 2115, 4418, 3004, 2153, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["example = \"This so-called \\\"Perfect Evening\\\" was so disappointing, as well as discouraging us from coming to your Circle Theatre again.\"\n","\n","embeddings = tokenizer(example)\n","embeddings"]},{"cell_type":"code","execution_count":9,"id":"033ecb19","metadata":{"id":"033ecb19","outputId":"7e35dc26-1379-45c5-d577-17385804908b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[CLS] this so - called \" perfect evening \" was so disappointing , as well as disco ##ura ##ging us from coming to your circle theatre again . [SEP]\n"]}],"source":["# TODO: Detokenization\n","decoded_tokens = tokenizer.batch_decode(embeddings[...])\n","print(' '.join(decoded_tokens))"]},{"cell_type":"code","execution_count":10,"id":"1118f333","metadata":{"id":"1118f333","outputId":"973e93eb-a4ed-4436-fed1-be140a6965f4"},"outputs":[{"data":{"text/plain":["{'input_ids': tensor([[  101,  2023,  2061,  1011,  2170,  1000,  3819,  3944,  1000,  2001,\n","          2061, 15640,  1010,  2004,  2092,  2004, 12532,  4648,  4726,  2149,\n","          2013,  2746,  2000,  2115,  4418,  3004,  2153,  1012,   102,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]])}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["# EXAMPLE: directly transform into embedding tensor\n","embeddings = tokenizer(\n","                       example,\n","                       max_length=128,\n","                       padding=\"max_length\",\n","                       is_split_into_words=False,\n","                       truncation=True,\n","                       return_tensors='pt',\n","                      )\n","embeddings"]},{"cell_type":"markdown","id":"0e667000","metadata":{"id":"0e667000"},"source":["### 3.4 Label processing\n","\n","In the following section, we will [OneHotEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder) provided by scikit-learn."]},{"cell_type":"code","execution_count":11,"id":"135b2f93","metadata":{"id":"135b2f93"},"outputs":[],"source":["import numpy as np\n","from sklearn.preprocessing import OneHotEncoder\n","\n","# First, declare a new encoder\n","encoder = OneHotEncoder(sparse = False)\n","\n","# Second, let the encoder learns all features in the given dataset\n","encoder = encoder.fit(np.reshape(dataset['train']['label'], (-1, 1)))"]},{"cell_type":"code","execution_count":12,"id":"3b71cc7f","metadata":{"id":"3b71cc7f","outputId":"9845b082-f71e-4c9e-f683-12b93263f83b"},"outputs":[{"data":{"text/plain":["3"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["LABEL_COUNT = len(encoder.categories_[0])\n","LABEL_COUNT"]},{"cell_type":"markdown","id":"068f7e29","metadata":{"id":"068f7e29"},"source":["### 3.5 Play with OneHotEncoder"]},{"cell_type":"markdown","id":"03b30ebc","metadata":{"id":"03b30ebc"},"source":["#### 3.5.1 Check what features has the encoder captured"]},{"cell_type":"code","execution_count":13,"id":"f4a89d9a","metadata":{"id":"f4a89d9a","outputId":"42de01f5-f769-43e7-d450-923e3b23e171"},"outputs":[{"name":"stdout","output_type":"stream","text":["[array(['negative', 'neutral', 'positive'], dtype='<U8')]\n"]}],"source":["print(encoder.categories_)"]},{"cell_type":"markdown","id":"299abbc4","metadata":{},"source":["#### 3.5.2 one-hot code\n","\n","Use `encoder.transform` to get the one-hot code of a label"]},{"cell_type":"code","execution_count":14,"id":"63ee709d","metadata":{"id":"63ee709d","outputId":"765b78fa-7134-4397-c009-55e3dab74f5b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0. 0. 1.]\n"," [1. 0. 0.]\n"," [0. 1. 0.]]\n"]}],"source":["# TODO: Check the code for each label\n","print(encoder.transform([[...], [...], [...]]))"]},{"cell_type":"markdown","id":"51422705","metadata":{},"source":["### 3.5.3 Decode one-hot code\n","\n","Use `encoder.inverse_transform` instead"]},{"cell_type":"code","execution_count":15,"id":"a9501dac","metadata":{"id":"a9501dac","outputId":"e9335e22-8cff-43e6-a868-85e9932a9cc0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[['positive']]\n"]}],"source":["print(encoder.inverse_transform([[0, 0, 1]]))"]},{"cell_type":"markdown","id":"72153561","metadata":{"id":"72153561"},"source":["### 3.6 Process the whole data\n","\n","With the `tokenizor` and `encoder` prepared, we can write a function to process the whole dataset."]},{"cell_type":"markdown","id":"d9fe4426","metadata":{},"source":["#### 3.6.1 Define preprocess function"]},{"cell_type":"code","execution_count":16,"id":"5669f4e7","metadata":{"id":"5669f4e7"},"outputs":[],"source":["def preprocess(dataslice):\n","    \"\"\"\n","    Input: a batch of your dataset\n","    Example: { 'text': [['sentence1'], ['setence2'], ...],\n","               'label': ['label1', 'label2', ...] }\n","    \"\"\"\n","\n","    embeddings = tokenizer(\n","                            dataslice['text'],\n","                            max_length=128,\n","                            padding=\"max_length\",\n","                            is_split_into_words=False,\n","                            truncation=True,\n","                            return_tensors='pt',\n","                          )\n","    labels = encoder.transform(np.reshape(dataslice['label'], (-1, 1)))\n","\n","    \"\"\"\n","    Output: a batch of processed dataset\n","    Example: {\n","                'input_ids': ...,\n","                'attention_masks': ...,\n","                'label': ...\n","              }\n","    \"\"\"\n","    return {**embeddings, 'label': labels}"]},{"cell_type":"markdown","id":"bc612333","metadata":{"id":"bc612333"},"source":["#### 3.6.2 Apply the preprocess function to the whole dataset"]},{"cell_type":"code","execution_count":17,"id":"caa298f0","metadata":{"id":"caa298f0","outputId":"5bb28058-f10a-4ae6-b7b4-5c7e98dcf69a"},"outputs":[{"name":"stderr","output_type":"stream","text":["Loading cached processed dataset at /home/nlplab/kedy/.cache/huggingface/datasets/csv/default-b6e650acb670b9ab/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317/cache-541a61ccb849e795.arrow\n"]}],"source":["processed_data = dataset.map(\n","                             preprocess,    # your processing function\n","                             batched = True # Process in batches so it can be faster\n","                            )"]},{"cell_type":"code","execution_count":18,"id":"74953918","metadata":{"id":"74953918","outputId":"64213829-5bb1-4e06-de6f-5ce085dec3a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['label', 'text', 'input_ids', 'attention_mask'],\n","        num_rows: 20632\n","    })\n","})\n"]},{"data":{"text/plain":["{'label': [0.0, 1.0, 0.0],\n"," 'text': \"Picturehouse's, Pink Floyd's, 'Roger Waters: The Walll - opening 29 Sept is now making waves. Watch the trailer on Rolling Stone - look...\",\n"," 'input_ids': [101,\n","  3861,\n","  4580,\n","  1005,\n","  1055,\n","  1010,\n","  5061,\n","  12305,\n","  1005,\n","  1055,\n","  1010,\n","  1005,\n","  5074,\n","  5380,\n","  1024,\n","  1996,\n","  2813,\n","  2140,\n","  1011,\n","  3098,\n","  2756,\n","  17419,\n","  2003,\n","  2085,\n","  2437,\n","  5975,\n","  1012,\n","  3422,\n","  1996,\n","  9117,\n","  2006,\n","  5291,\n","  2962,\n","  1011,\n","  2298,\n","  1012,\n","  1012,\n","  1012,\n","  102,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0],\n"," 'attention_mask': [1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  1,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0,\n","  0]}"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Take a look at processed dataset\n","print(processed_data)\n","processed_data['train'][0]"]},{"cell_type":"markdown","id":"f670f7c6","metadata":{"id":"f670f7c6"},"source":["### 3.7 DataCollator\n","\n","To do the training-time processing, we can use the DataCollator Class provided by `transformers`.\n","\n"," - [transformers.DataCollatorWithPadding](https://huggingface.co/docs/transformers/master/en/main_classes/data_collator#transformers.DataCollatorWithPadding)"]},{"cell_type":"code","execution_count":19,"id":"1add1e45","metadata":{"id":"1add1e45"},"outputs":[],"source":["from transformers import DataCollatorWithPadding\n","\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"]},{"cell_type":"markdown","id":"be824b94","metadata":{"id":"be824b94"},"source":["## Step 4: Training"]},{"cell_type":"markdown","id":"57154197","metadata":{"id":"57154197"},"source":["### Preparation\n","\n","We can load the pretrained model from `transformers`.\n","\n","Generally, you need to build your own model on top of BERT if you want to use BERT for some downstream tasks.\n","\n","Fortunately, sequence classification is a popular topic. With the support from `transformers` library, all works can be done in two lines of codes: \n","\n","1. Load `AutoModelForSequenceClassification` Class.\n","2. Load the pretrained model."]},{"cell_type":"markdown","id":"d905ecdb","metadata":{},"source":["#### 4.1 Load `BERT` model"]},{"cell_type":"code","execution_count":20,"id":"6a09afeb","metadata":{"id":"6a09afeb","outputId":"ab996f02-c918-40bd-c889-cd9cefb72b4c"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n","- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","\n","model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME,\n","                                                           num_labels = LABEL_COUNT)"]},{"cell_type":"markdown","id":"59d085bc","metadata":{"id":"59d085bc"},"source":["### 4.2 Split train/val data\n","\n","The `Dataset` class we prepared before already has the `train_test_split` method. You can use it to split your dataset.\n","\n","Document:\n"," - [datasets.Dataset - Sort, shuffle, select, split, and shard](https://huggingface.co/docs/datasets/process.html#sort-shuffle-select-split-and-shard)\n"]},{"cell_type":"code","execution_count":21,"id":"9352c946","metadata":{"id":"9352c946"},"outputs":[],"source":["# [ TODO ] Choose the validation data size\n","train_val_dataset = processed_data['train'].train_test_split(test_size = ...)"]},{"cell_type":"code","execution_count":22,"id":"545bc915","metadata":{"id":"545bc915","outputId":"dd3a612d-8581-4a8d-a390-941f0e8b73db"},"outputs":[{"name":"stdout","output_type":"stream","text":["DatasetDict({\n","    train: Dataset({\n","        features: ['label', 'text', 'input_ids', 'attention_mask'],\n","        num_rows: 18568\n","    })\n","    test: Dataset({\n","        features: ['label', 'text', 'input_ids', 'attention_mask'],\n","        num_rows: 2064\n","    })\n","})\n"]}],"source":["# Take a look at split data\n","print(train_val_dataset)"]},{"cell_type":"markdown","id":"a0667bc9","metadata":{"id":"a0667bc9"},"source":["#### 4.3 Setup training parameters"]},{"cell_type":"markdown","id":"cce19cb5","metadata":{},"source":["#### 4.3.1 Import `TrainingArguments`, `Trainer` from `transformers`"]},{"cell_type":"code","execution_count":23,"id":"189a7144","metadata":{"id":"189a7144"},"outputs":[],"source":["from transformers import TrainingArguments, Trainer"]},{"cell_type":"markdown","id":"37d08edc","metadata":{},"source":["#### 4.3.2 Set training properties and initialize a trainer"]},{"cell_type":"code","execution_count":24,"id":"8bf81339","metadata":{"id":"8bf81339"},"outputs":[],"source":["# TODO: Setup training details\n","BATCH_SIZE = ...\n","EPOCH = ...\n","MODEL_FOLDER = \"finetuned\"\n","training_args = TrainingArguments(\n","    output_dir = f\"./model/{MODEL_FOLDER}\",\n","    learning_rate = 2e-4,\n","    per_device_train_batch_size = BATCH_SIZE,\n","    per_device_eval_batch_size = BATCH_SIZE,\n","    num_train_epochs = EPOCH,\n","    # You can also set other parameters here\n",")\n","\n","trainer = Trainer(\n","    model = model,\n","    args = training_args,\n","    train_dataset = train_val_dataset[\"train\"],\n","    eval_dataset = train_val_dataset[\"test\"],\n","    tokenizer = tokenizer,\n","    data_collator = data_collator,\n","    # You can also set other parameters\n",")"]},{"cell_type":"markdown","id":"4abd2b6d","metadata":{"id":"4abd2b6d"},"source":["### 4.4 Training\n","\n","Training is pretty easy. Simply ask the trainer to train the model for you!"]},{"cell_type":"code","execution_count":25,"id":"bae54732","metadata":{"id":"bae54732","outputId":"baee379b-8152-4345-89c3-3fdd7d5348ac"},"outputs":[{"name":"stderr","output_type":"stream","text":["The following columns in the training set don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n","/home/nlplab/kedy/miniconda3/envs/sentiment-analysis/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","***** Running training *****\n","  Num examples = 18568\n","  Num Epochs = 1\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1161\n","  Number of trainable parameters = 66955779\n","You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1161' max='1161' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1161/1161 01:54, Epoch 1/1]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>0.559500</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.508700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Saving model checkpoint to ./model/finetuned/checkpoint-500\n","Configuration saved in ./model/finetuned/checkpoint-500/config.json\n","Model weights saved in ./model/finetuned/checkpoint-500/pytorch_model.bin\n","tokenizer config file saved in ./model/finetuned/checkpoint-500/tokenizer_config.json\n","Special tokens file saved in ./model/finetuned/checkpoint-500/special_tokens_map.json\n","Saving model checkpoint to ./model/finetuned/checkpoint-1000\n","Configuration saved in ./model/finetuned/checkpoint-1000/config.json\n","Model weights saved in ./model/finetuned/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ./model/finetuned/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ./model/finetuned/checkpoint-1000/special_tokens_map.json\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"data":{"text/plain":["TrainOutput(global_step=1161, training_loss=0.5272752628769986, metrics={'train_runtime': 114.5589, 'train_samples_per_second': 162.083, 'train_steps_per_second': 10.135, 'total_flos': 614924630673408.0, 'train_loss': 0.5272752628769986, 'epoch': 1.0})"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["trainer.train()"]},{"cell_type":"markdown","id":"8e63e5b3","metadata":{"id":"8e63e5b3"},"source":["You can see that Trainer saves some ckeckpoints, so you can load your model from those checkpoints if you want to fallback to a specific version."]},{"cell_type":"markdown","id":"b1501549","metadata":{"id":"b1501549"},"source":["### 4.5 Save for future use"]},{"cell_type":"code","execution_count":26,"id":"d400687f","metadata":{"id":"d400687f","outputId":"ab70c7c9-bad3-4d99-bd9d-31248007d415"},"outputs":[{"name":"stderr","output_type":"stream","text":["Configuration saved in model/finetuned/config.json\n","Model weights saved in model/finetuned/pytorch_model.bin\n"]}],"source":["model.save_pretrained(os.path.join('model', 'finetuned'))"]},{"cell_type":"markdown","id":"5af492d0","metadata":{"id":"5af492d0"},"source":["## Step 5: Prediction"]},{"cell_type":"markdown","id":"5587893d","metadata":{"id":"5587893d"},"source":["### 5.1 Load finetuned model"]},{"cell_type":"code","execution_count":27,"id":"71dc129d","metadata":{"id":"71dc129d"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file model/finetuned/config.json\n","Model config DistilBertConfig {\n","  \"_name_or_path\": \"model/finetuned\",\n","  \"activation\": \"gelu\",\n","  \"architectures\": [\n","    \"DistilBertForSequenceClassification\"\n","  ],\n","  \"attention_dropout\": 0.1,\n","  \"dim\": 768,\n","  \"dropout\": 0.1,\n","  \"hidden_dim\": 3072,\n","  \"id2label\": {\n","    \"0\": \"LABEL_0\",\n","    \"1\": \"LABEL_1\",\n","    \"2\": \"LABEL_2\"\n","  },\n","  \"initializer_range\": 0.02,\n","  \"label2id\": {\n","    \"LABEL_0\": 0,\n","    \"LABEL_1\": 1,\n","    \"LABEL_2\": 2\n","  },\n","  \"max_position_embeddings\": 512,\n","  \"model_type\": \"distilbert\",\n","  \"n_heads\": 12,\n","  \"n_layers\": 6,\n","  \"pad_token_id\": 0,\n","  \"problem_type\": \"multi_label_classification\",\n","  \"qa_dropout\": 0.1,\n","  \"seq_classif_dropout\": 0.2,\n","  \"sinusoidal_pos_embds\": false,\n","  \"tie_weights_\": true,\n","  \"torch_dtype\": \"float32\",\n","  \"transformers_version\": \"4.24.0\",\n","  \"vocab_size\": 30522\n","}\n","\n","loading weights file model/finetuned/pytorch_model.bin\n","All model checkpoint weights were used when initializing DistilBertForSequenceClassification.\n","\n","All the weights of DistilBertForSequenceClassification were initialized from the model checkpoint at model/finetuned.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertForSequenceClassification for predictions without further training.\n"]}],"source":["from transformers import AutoModelForSequenceClassification\n","\n","mymodel = AutoModelForSequenceClassification.from_pretrained(os.path.join('model', 'finetuned'))"]},{"cell_type":"markdown","id":"1583078f","metadata":{"id":"1583078f"},"source":["### 5.2 Get the prediction"]},{"cell_type":"code","execution_count":28,"id":"5623b71a","metadata":{"id":"5623b71a"},"outputs":[],"source":["examples = [\n","    # neutral\n","    \"All of you people who're saying The Weekend is the next Michael Jackson, Go to sleep, you got school tomorrow.\",\n","    # negative\n","    \"Here I am saying Kanye isn't a bad guy when these people just gave you a fucking award that Michael Jackson got c'mon bro\",\n","    # positive\n","    \"@MariahCarey may he R.I.P. Happy Birthday Michael Jackson . :)\",\n","]"]},{"cell_type":"code","execution_count":29,"id":"e2276924","metadata":{"id":"e2276924","outputId":"7603b5f7-de9b-4fdd-8c76-2bb4f67cb153"},"outputs":[{"data":{"text/plain":["tensor([[-2.2525, -0.4102,  0.0044],\n","        [-0.0196, -0.5089, -2.0296],\n","        [-3.4792, -1.5006,  1.2738]], grad_fn=<AddmmBackward0>)"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# Transform the sentences into embeddings\n","input = tokenizer(examples, truncation=True, padding=True, return_tensors=\"pt\")\n","# Get the output\n","logits = mymodel(**input).logits\n","logits"]},{"cell_type":"markdown","id":"575403a7","metadata":{},"source":["### 5.3 Transform logits with softmax activation\n","\n","Use softmax activation to transform them into more probability-like numbers"]},{"cell_type":"code","execution_count":30,"id":"a2ea86b4","metadata":{"id":"a2ea86b4","outputId":"75cfc2c8-26a3-411d-ab55-1ef6818b3e79"},"outputs":[{"data":{"text/plain":["tensor([[0.0593, 0.3742, 0.5665],\n","        [0.5724, 0.3509, 0.0767],\n","        [0.0081, 0.0583, 0.9337]], grad_fn=<SoftmaxBackward0>)"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["from torch import nn\n","\n","predicts = nn.functional.softmax(logits, dim = -1)\n","predicts"]},{"cell_type":"markdown","id":"c0a08934","metadata":{"id":"c0a08934"},"source":["### 5.4 Transform logits back to labels"]},{"cell_type":"code","execution_count":31,"id":"f6fba1d4","metadata":{"id":"f6fba1d4"},"outputs":[],"source":["def predict2label(logits, input):\n","    predictions = np.zeros(shape=(len(input['input_ids']), LABEL_COUNT))\n","    predict_id = logits.argmax(dim = 1)\n","    predictions[np.arange(predict_id.size()[0]), predict_id] = 1\n","    return predict_id, encoder.inverse_transform(predictions).flatten()"]},{"cell_type":"code","execution_count":32,"id":"942e676c","metadata":{"id":"942e676c","outputId":"43ecb825-0991-4b4f-93d2-403ee7781cc6"},"outputs":[{"data":{"text/plain":["(tensor([2, 0, 2]), array(['positive', 'negative', 'positive'], dtype='<U8'))"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["predict2label(predicts, input)"]},{"cell_type":"markdown","id":"86ec453e","metadata":{"id":"86ec453e"},"source":["## Step 6: Evaluation\n","\n","Load the testing data and calculate your accuracy.\n"]},{"cell_type":"markdown","id":"f778025d","metadata":{},"source":["#### 6.1 Load test data"]},{"cell_type":"code","execution_count":34,"id":"2d6f47db","metadata":{"colab":{"referenced_widgets":["30b33af77fee4bf8b515f860fb4f5d72"]},"id":"2d6f47db","outputId":"9baa535d-414a-4c4d-c2fc-ffe55984c993"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using custom data configuration default-2b08c652a1924c67\n","Found cached dataset csv (/home/nlplab/kedy/.cache/huggingface/datasets/csv/default-2b08c652a1924c67/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n","100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 330.18it/s]\n"]}],"source":["test_data = load_dataset('csv', data_files = os.path.join('data', 'test.csv'))['train']\n","input = tokenizer(test_data['text'], truncation = True, padding = True, return_tensors = 'pt')"]},{"cell_type":"code","execution_count":35,"id":"d97368b9","metadata":{"id":"d97368b9"},"outputs":[],"source":["logits = mymodel(**input).logits"]},{"cell_type":"code","execution_count":36,"id":"d65efdfb","metadata":{"id":"d65efdfb"},"outputs":[],"source":["predicts = nn.functional.softmax(logits, dim = -1)"]},{"cell_type":"code","execution_count":37,"id":"40f80022","metadata":{"id":"40f80022"},"outputs":[],"source":["predict_id, predict_label = predict2label(predicts, input)"]},{"cell_type":"code","execution_count":38,"id":"61f238ec","metadata":{"id":"61f238ec","outputId":"dddc9d23-d9b1-4f78-d68c-08ffdb44c642"},"outputs":[{"name":"stdout","output_type":"stream","text":["neutral: 05 Beat it - Michael Jackson - Thriller (25th Anniversary Edition) [HD] http://t.co/A4K2B86PBv\n","neutral: Jay Z joins Instagram with nostalgic tribute to Michael Jackson: Jay Z apparently joined Instagram on Saturday and.. http://t.co/Qj9I4eCvXy\n","positive: Michael Jackson: Bad 25th Anniversary Edition (Picture Vinyl): This unique picture disc vinyl includes the original 1 http://t.co/fKXhToAAuW\n","positive: I liked a @YouTube video http://t.co/AaR3pjp2PI One Direction singing \"Man in the Mirror\" by Michael Jackson in Atlanta, GA [June 26,\n","neutral: 18th anniv of Princess Diana's death. I still want to believe she is living on a private island away from the public. With Michael Jackson.\n"]}],"source":["for idx, (sent, label) in enumerate(zip(test_data['text'], predict_label)):\n","    if idx >= 5: break\n","    print(f'{label}: {sent}')"]},{"cell_type":"markdown","id":"e4cef45a","metadata":{"id":"e4cef45a"},"source":["### Accuracy\n","\n","$\n","accuracy = \\frac{\\#exactly\\:the\\:same\\:levels}{\\#total}\n","$\n","\n","Example:\n","```\n","Prediction:   Pos Pos Neg Neu Neu\n","Ground truth: Pos Neu Neg Neu Neg\n","               ^       ^   ^\n","```\n","\n","The six level accuracy is $\\frac{3}{5} = 0.6$"]},{"cell_type":"code","execution_count":39,"id":"13bdf14c","metadata":{"id":"13bdf14c"},"outputs":[],"source":["correct = 0\n","total = 0\n","\n","for predict, label in zip(predict_label, test_data['label']):\n","    if predict == label:\n","        correct += 1\n","    total += 1"]},{"cell_type":"code","execution_count":40,"id":"6545d339","metadata":{"id":"6545d339","outputId":"d8f1cbe8-9f9c-4029-e637-123812c52544"},"outputs":[{"data":{"text/plain":["0.5571142284569138"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["accuracy = correct / total\n","accuracy"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 ('sentiment-analysis')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"2d4f785791bbe0e47942ff647855d9744a9162498c4070dd60930fc2b7b1e4fc"}}},"nbformat":4,"nbformat_minor":5}
